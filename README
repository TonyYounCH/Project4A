NAME: Changhui Youn
EMAIL: tonyyoun2@gmail.com
ID: 304207830

Project4B is divided into parts :
	- Do performance instrumentation and measurement to confirm the cause of the problem.
	- Implement a new option to divide a list into sublists and support synchronization on sublists, thus allowing parallel access to the (original) list.
	- Do new performance measurements to confirm that the problem has been solved.


##################
#  lab2_list.c   #
##################

lab2_add accpets four options : --threads=#, --iterations=#, --yield=[idl], --sync=[m,s], --lists=#

It processes arguments and retrieve threads, iterations, and list numbers to create threads. 
Based on yield and sync option, threads will perform insert/delete/lookup to doubly linked list wrapped with mutex lock or spin lock. 

Mutex-lock or spin-lock will be initialized num_list times and each thread will have its hash value to obtain/release the lock. 

After all operations are done, it will print out :

test, threads, iterations, list, num_operations, run_time, avg_per_op


##################
#  SortedList.h  #
##################

a header file containing interfaces for linked list operations.

##################
#  SortedList.c  #
##################

Implement all four methods described in SortedList header file. Interface includes three software-conterolled yield options. 
Identify the critical section in each of four methods and add calls to pthread_yield or sched_yield, controlled by the yield options:

in SortedList_insert if opt_yield & INSERT_YIELD
in SortedList_delete if opt_yield & DELETE_YIELD
in SortedList_lookup if opt_yield & LOOKUP_YIELD
in SortedList_length if opt_yield & LOOKUP_YIELD


##################
#  lab2_list.gp  #
##################

Graph generating script using gnuplot

#################
#     Graph     #
#################

	lab2b_1.png : throughput vs. number of threads for mutex and spin-lock synchronized list operations.

	lab2b_2.png : mean time per mutex wait and mean time per operation for mutex-synchronized list operations.

	lab2b_3.png : successful iterations vs. threads for each synchronization method.

	lab2b_4.png : throughput vs. number of threads for mutex synchronized partitioned lists.

	lab2b_5.png : throughput vs. number of threads for spin-lock-synchronized partitioned lists.

#################
#      CSV      #
#################

lab2b_list.csv : containing all of results for all of the lab2_list tests.


##################
#   profile.out  #
##################

Execution profiling report showing where time was spent in the un-partitioned spin-lock implementation.

##################
#    Makefile    #
##################

default	: the lab2_list executable (compiling with the -Wall and -Wextra options)
tests 	: run all specified test cases to generate CSV results
profile : run tests with profiling tools to generate an execution profiling report 
graphs 	: use gnuplot to generate the required graphs
dist 	: create the deliverable tarball
clean 	: delete all programs and output generated by the Makefile

##################
#    tests.sh    #
##################

Tests script that runs all tests for lab2_list

##################
#     README     #
##################

Descriptions of each of the included files and any other information about my submission that I would like to bring to attention (e.g. research, limitations, features, testing methodology).

QUESTIONS

QUESTION 2.3.1 - CPU time in the basic list implementation
	
	Q. Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?

	A. For 1-thread and 2-thread tests, most of the CPU time would be spent doing list operations.

	Q. Why do you believe these to be the most expensive parts of the code?

	A. Because the time that threads spend while waiting for the resource would be very small (smaller than time it spent doing list operations).

	Q. Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests? Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

	A. For high-thread spin-lock tests, most of the CPU time will be spent spinning as threads spin until the resource is free. 
	For high-thread mutex tests, most of the CPU time will be spent on mutex functions as threads will wait for the mutex-lock to be unlocked.  

QUESTION 2.3.2 - Execution Profiling

	Q. Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?

	A. 
		while(__sync_lock_test_and_set(&spin_lock[hash], 1));
		while(__sync_lock_test_and_set(&spin_lock[i], 1));

	Q. Why does this operation become so expensive with large numbers of threads?

	A. These lines of code are consuming most of the CPU time and become expensive with large numbers of threads because there will be higher contention for the lock. As number of threads increases, more threads will be spinning waiting for the resources to be released.

QUESTION 2.3.3 - Mutex Wait Time

	Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
		
	Q. Why does the average lock-wait time rise so dramatically with the number of contending threads?

	A. As number of threads increase, more threads are forced to wait for the lock. Compared to 1-thread where it does not need to wait for the resource to be unlocked, more threads will need to contend, resulting average lock-wait time to rise dramatically.

	Q. Why does the completion time per operation rise (less dramatically) with the number of contending threads?

	A. It also rises (less dramatically) because more threads will need to be put to sleep and waken up which increases overheads. Howeve, since each thread that is using the resource is completing its job, at least one thread is making its progress. This leads to a less dramatic rise on completion time per operation.

	Q. How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

	It is possible due to the fact that wait time of threads can overlap while completion time can't. Multiple threads can wait for the resource at the same time but only one thread can actually do its job at specific time. 

QUESTION 2.3.4 - Performance of Partitioned Lists

	Q. Explain the change in performance of the synchronized methods as a function of the number of lists.

	A. As the number of lists increases the throughput increases.

	Q. Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

	A. The throughput will continue to increase as the number of lists increases until certain point. If it reaches to the point where each thread will not need to wait for another thread, increasing the number of lists will have no effect on the throughput. 

	Q. It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

	A. They seem to be equivalent but not exactly. The points corresponding to the throughput are not exactly the same. As the number of sublist increases, the length of each sublist may differ slightly which leads to difference in accessing the resource. 

Reference 

Skeleton code from Discussion 1A/B. 
https://drive.google.com/drive/folders/0B6dyEb8VXZo-N3hVcVI0UFpWSVk

